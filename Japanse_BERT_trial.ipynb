{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Japanse_BERT_trial.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMTPIhaA5Z1ADF5ORlda6Bd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kiitaamuuraa/Asobiba/blob/main/Japanse_BERT_trial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yZb292IQWX8",
        "outputId": "ca4991d8-1bf8-43c5-9e11-15e004460666"
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/54/5ca07ec9569d2f232f3166de5457b63943882f7950ddfcc887732fc7fb23/transformers-4.3.3-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 6.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 22.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 41.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=a671236514b2d59f9e747bb6795c84401d50196921ab329245dc8397aba6d131\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQeQcFSBQ3Wv",
        "outputId": "80a106aa-63a7-4614-82b2-30a719696718"
      },
      "source": [
        "!pip install transformers[\"ja\"]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers[ja] in /usr/local/lib/python3.7/dist-packages (4.3.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (3.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (0.0.43)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (20.9)\n",
            "Collecting unidic-lite>=1.0.7; extra == \"ja\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/2b/8cf7514cb57d028abcef625afa847d60ff1ffbf0049c36b78faa7c35046f/unidic-lite-1.0.8.tar.gz (47.4MB)\n",
            "\u001b[K     |████████████████████████████████| 47.4MB 61kB/s \n",
            "\u001b[?25hCollecting unidic>=1.0.2; extra == \"ja\"\n",
            "  Downloading https://files.pythonhosted.org/packages/86/04/c18832fd9959a78fc60eeaa9e7fb37ef31a250e8645cc2897eb1f07939ee/unidic-1.0.3.tar.gz\n",
            "Collecting ipadic<2.0,>=1.0.0; extra == \"ja\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/4e/c459f94d62a0bef89f866857bc51b9105aff236b83928618315b41a26b7b/ipadic-1.0.0.tar.gz (13.4MB)\n",
            "\u001b[K     |████████████████████████████████| 13.4MB 242kB/s \n",
            "\u001b[?25hCollecting fugashi>=1.0; extra == \"ja\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/9c/009da34dd111e84f54eef833c84afb5c744a0306af8546014a958e1967a0/fugashi-1.1.0-cp37-cp37m-manylinux1_x86_64.whl (486kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 36.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers[ja]) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers[ja]) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[ja]) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[ja]) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[ja]) (1.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[ja]) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[ja]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[ja]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[ja]) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers[ja]) (2.4.7)\n",
            "Requirement already satisfied: wasabi<1.0.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from unidic>=1.0.2; extra == \"ja\"->transformers[ja]) (0.8.2)\n",
            "Requirement already satisfied: plac<2.0.0,>=1.1.3 in /usr/local/lib/python3.7/dist-packages (from unidic>=1.0.2; extra == \"ja\"->transformers[ja]) (1.1.3)\n",
            "Building wheels for collected packages: unidic-lite, unidic, ipadic\n",
            "  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-cp37-none-any.whl size=47658825 sha256=3d4d5f9a2be11b47c743e0972faa585f7f71c382de6909543afe3cce8d081139\n",
            "  Stored in directory: /root/.cache/pip/wheels/20/48/8d/b66d8361a27f58f41ec86640e4fd2640de0403a6367511eab7\n",
            "  Building wheel for unidic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unidic: filename=unidic-1.0.3-cp37-none-any.whl size=5497 sha256=b7eae0b41dbab3c0ab5b11e26a6ea34765feff6f9b22aafe951bad961666114a\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/26/e2/fb76c79fd14391eb994eab021c9129c24814125298e1e5b96a\n",
            "  Building wheel for ipadic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipadic: filename=ipadic-1.0.0-cp37-none-any.whl size=13556725 sha256=c6dfd8b847373504900d28cdc66167349f8d7299c3be6a476791abcb6d6157de\n",
            "  Stored in directory: /root/.cache/pip/wheels/ff/00/d1/0c094a0ce58a77199a0c5801f0ecf510c80f0ecbec27f07d2c\n",
            "Successfully built unidic-lite unidic ipadic\n",
            "Installing collected packages: unidic-lite, unidic, ipadic, fugashi\n",
            "Successfully installed fugashi-1.1.0 ipadic-1.0.0 unidic-1.0.3 unidic-lite-1.0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBCNFo9LQXAo"
      },
      "source": [
        "# PyTorch\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "\r\n",
        "# transformers\r\n",
        "from transformers import BertModel\r\n",
        "from transformers import BertJapaneseTokenizer"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7xiIn2xtFbB"
      },
      "source": [
        "## 1. インスタンス作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acswNsFGRgGJ"
      },
      "source": [
        "# トークナイザー\r\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese')\r\n",
        "# モデルインスタンス\r\n",
        "bert = BertModel.from_pretrained('cl-tohoku/bert-base-japanese')"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVWI4rCxtLui"
      },
      "source": [
        "## 2. 前処理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYwTCT3gShUQ",
        "outputId": "dd6263ce-f67e-443d-d510-fdd7fdb18347"
      },
      "source": [
        "kanikosen= ['「おい地獄さ行ぐんだで！」',\\\r\n",
        "            '二人はデッキの手すりに寄りかかって、蝸牛が背のびをしたように延びて、海を抱かえ込んでいる函館の街を見ていた。',\\\r\n",
        "            '――漁夫は指元まで吸いつくした煙草を唾と一緒に捨てた。']\r\n",
        "\r\n",
        "# トークナイズ\r\n",
        "# 純粋にトークナイズした結果で、BERTの入力形式ではないので注意\r\n",
        "# tokeneizerメソッドはバッチを受け付けないので注意\r\n",
        "\r\n",
        "for sent in kanikosen:\r\n",
        "    print(tokenizer.tokenize(sent))\r\n",
        "    print(len(sent), '\\n')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['「', 'おい', '地獄', 'さ', '行', 'ぐん', '##だ', '##で', '!」']\n",
            "13 \n",
            "\n",
            "['二', '人', 'は', 'デッキ', 'の', '手', '##すり', 'に', '寄り', '##かかっ', 'て', '、', '[UNK]', 'が', '背', '##の', '##び', 'を', 'し', 'た', 'よう', 'に', '延び', 'て', '、', '海', 'を', '抱か', 'え', '込ん', 'で', 'いる', '函館', 'の', '街', 'を', '見', 'て', 'い', 'た', '。']\n",
            "54 \n",
            "\n",
            "['――', '漁', '##夫', 'は', '指', '元', 'まで', '吸い', 'つく', '##し', 'た', '煙草', 'を', '[UNK]', 'と', '一緒', 'に', '捨て', 'た', '。']\n",
            "27 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bo0tcZ9jStzJ"
      },
      "source": [
        "# BERT入力形式に変換\r\n",
        "tokenized = tokenizer(\\\r\n",
        "    kanikosen,             # 入力文\r\n",
        "    padding=True,          # 入力文集合中の最大長に合わせてパディング\r\n",
        "    truncation=True,       # 長すぎる文を、以下のmax_lengthのサイズに合わせてカットするか\r\n",
        "    max_length=512,\r\n",
        "    return_tensors=\"pt\"    # 返却する型: 省略時はリスト、他はtf,np\r\n",
        "    )"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MU23nB1pxLgn",
        "outputId": "343cf3c2-538b-4c84-9a51-13000f4ab186"
      },
      "source": [
        "# 確認\r\n",
        "for k, v in tokenized.items():\r\n",
        "    print('key: ', k)\r\n",
        "    print('value: ', v, '\\n')\r\n",
        "\r\n",
        "for sent in tokenized['input_ids']:\r\n",
        "    print(len(sent))\r\n",
        "decoded = str()\r\n",
        "\r\n",
        "for id in tokenized['input_ids'][0]:\r\n",
        "    decoded += tokenizer.decode(id)\r\n",
        "print(decoded)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "key:  input_ids\n",
            "value:  tensor([[    2,    36,  7613,  9867,    26,    77, 11043, 28565, 28455,  3286,\n",
            "             3,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0],\n",
            "        [    2,   287,    53,     9, 12824,     5,   319, 26853,     7,  6562,\n",
            "         24645,    16,     6,     1,    14,  1503, 28444, 28670,    11,    15,\n",
            "            10,   124,     7, 13081,    16,     6,   295,    11, 27978,  1723,\n",
            "         19736,    12,    33,  8159,     5,  1243,    11,   212,    16,    21,\n",
            "            10,     8,     3],\n",
            "        [    2, 18454,  3114, 29219,     9,   254,   281,   126, 16470,  2950,\n",
            "         28454,    10, 24168,    11,     1,    13,  4265,     7,  6638,    10,\n",
            "             8,     3,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0]]) \n",
            "\n",
            "key:  token_type_ids\n",
            "value:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]) \n",
            "\n",
            "key:  attention_mask\n",
            "value:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]) \n",
            "\n",
            "43\n",
            "43\n",
            "43\n",
            "[ C L S ]「お い地 獄さ行ぐ ん# # だ# # で! 」[ S E P ][ P A D ][ P A D ][ P A D ][ P A D ][ P A D ][ P A D ][ P A D ][ P A D ][ P A D ][ P A D ][ P A D ][ P A D ][ P A D ][ P A D ][ P A D ][ P A D ][ P A D ][ P A D ][ P A D ][ P A D ][ P A D ][ P A D ][ P A D ][ P A D ][ P A D ][ P A D ][ P A D ][ P A D ][ P A D ][ P A D ][ P A D ][ P A D ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuOVDU0exYC9"
      },
      "source": [
        "## 3. 学習"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwoDAvh_x6Jx"
      },
      "source": [
        "### 構成概要\r\n",
        "Embedding層 → BERT層 → pool層  \r\n",
        "以下は、 ```print(bert)``` の出力結果の概要\r\n",
        "\r\n",
        "### Embedding層\r\n",
        "\r\n",
        "```python\r\n",
        "  (embeddings): BertEmbeddings(\r\n",
        "    (word_embeddings): Embedding(32000, 768, padding_idx=0)\r\n",
        "    (position_embeddings): Embedding(512, 768)\r\n",
        "    (token_type_embeddings): Embedding(2, 768)\r\n",
        "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r\n",
        "    (dropout): Dropout(p=0.1, inplace=False)\r\n",
        "  )\r\n",
        "```\r\n",
        "### BERT層\r\n",
        "以下の層が12積層\r\n",
        "```python\r\n",
        "      (0): BertLayer(\r\n",
        "        (attention): BertAttention(\r\n",
        "          (self): BertSelfAttention(\r\n",
        "            (query): Linear(in_features=768, out_features=768, bias=True)\r\n",
        "            (key): Linear(in_features=768, out_features=768, bias=True)\r\n",
        "            (value): Linear(in_features=768, out_features=768, bias=True)\r\n",
        "            (dropout): Dropout(p=0.1, inplace=False)\r\n",
        "          )\r\n",
        "          (output): BertSelfOutput(\r\n",
        "            (dense): Linear(in_features=768, out_features=768, bias=True)\r\n",
        "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r\n",
        "            (dropout): Dropout(p=0.1, inplace=False)\r\n",
        "          )\r\n",
        "        )\r\n",
        "        (intermediate): BertIntermediate(\r\n",
        "          (dense): Linear(in_features=768, out_features=3072, bias=True)\r\n",
        "        )\r\n",
        "        (output): BertOutput(\r\n",
        "          (dense): Linear(in_features=3072, out_features=768, bias=True)\r\n",
        "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r\n",
        "          (dropout): Dropout(p=0.1, inplace=False)\r\n",
        "        )\r\n",
        "      )\r\n",
        "```\r\n",
        "### pool層\r\n",
        "\r\n",
        "```python\r\n",
        "\r\n",
        "  (pooler): BertPooler(\r\n",
        "    (dense): Linear(in_features=768, out_features=768, bias=True)\r\n",
        "    (activation): Tanh()\r\n",
        "  )\r\n",
        ")\r\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95KGLb8f9Quk"
      },
      "source": [
        "# pool層に分類層を追加\r\n",
        "bert.pooler = nn.Sequential(bert.pooler, nn.Linear(768, 3))"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBnUiZTZ949j",
        "outputId": "d77a5206-ceb8-477d-fd11-72f7ea25509c"
      },
      "source": [
        "print(bert.pooler)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): BertPooler(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (activation): Tanh()\n",
            "  )\n",
            "  (1): Linear(in_features=768, out_features=3, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dir_5YlLplPg"
      },
      "source": [
        "# モデルを学習モードに切り替え\r\n",
        "bert.train()\r\n",
        "\r\n",
        "outputs = bert(tokenized['input_ids'], attention_mask=tokenized['attention_mask'])"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TylnrzMWpps9",
        "outputId": "a43b4a1c-0147-46c5-a03f-0f537ca2d164"
      },
      "source": [
        "for k, v in outputs.items():\r\n",
        "    print('key: ', k, 'shape: ', v.shape)\r\n",
        "print(outputs[1])"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "key:  last_hidden_state shape:  torch.Size([3, 43, 768])\n",
            "key:  pooler_output shape:  torch.Size([3, 3])\n",
            "tensor([[ 0.2459, -0.1721, -0.0784],\n",
            "        [ 0.1571, -0.1872,  0.0925],\n",
            "        [ 0.2502, -0.2422, -0.2857]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTPMtbf4rhWf"
      },
      "source": [
        "# オプティマイザ\r\n",
        "from transformers import AdamW\r\n",
        "optimizer = AdamW(bert.parameters(), lr=1e-5)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRj2ssrB8Pme",
        "outputId": "35b1f368-9218-4ab6-8cd3-c575a885049d"
      },
      "source": [
        "# ここから先は、いつものPyTorch（もろもろ省略はしている）\r\n",
        "labels = torch.tensor([0, 1, 2])\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "loss = criterion(outputs[1], labels)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.1693, grad_fn=<NllLossBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBkPjayO_HsK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}